# SageMaker Timeout Fix - Complete Audit Report

**Date:** November 7, 2025  
**Issue:** ModelError timeout during CLIP model cold start (2-3 minute load time exceeds default 60s timeout)  
**Status:** ‚úÖ **ALL DEPLOYMENT SCRIPTS FIXED**

---

## Problem Summary

The StyGig Fashion Recommendation System uses a large CLIP model (ViT-B-32) that takes 2-3 minutes (120-180 seconds) to load in the `model_fn()` function during cold start. This exceeds the default SageMaker server timeout of 60 seconds, causing this error:

```
ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: 
Received server error (0) from primary with message "Your invocation timed out while 
waiting for a response from container primary..."
```

---

## Solution Applied

All deployment scripts have been updated with **TWO CRITICAL TIMEOUT SETTINGS**:

### 1. Server Timeout (5 Minutes)
Set via environment variables in the `PyTorchModel` constructor:

```python
model = PyTorchModel(
    # ... other parameters ...
    env={
        'SAGEMAKER_MODEL_SERVER_TIMEOUT': '300',  # ‚≠ê CRITICAL: SageMaker-specific timeout
        'MODEL_SERVER_TIMEOUT': '300',            # 5 minutes per request
        'TS_DEFAULT_RESPONSE_TIMEOUT': '300',     # TorchServe timeout
        'MODEL_SERVER_WORKERS': '1',
        'TS_DEFAULT_WORKERS_PER_MODEL': '1',
        # ... optimization flags ...
    }
)
```

### 2. Container Startup Timeout (10 Minutes)
Set via parameters in the `.deploy()` method:

```python
predictor = model.deploy(
    # ... other parameters ...
    container_startup_health_check_timeout=600,  # ‚≠ê CRITICAL: 10 min for first startup
    model_data_download_timeout=600              # 10 min to download model from S3
)
```

---

## Files Audited and Fixed

### ‚úÖ **Already Correct** (Reference Implementations)

#### 1. `sagemaker/redeploy_with_timeout.py`
- **Status:** ‚úÖ Perfect reference implementation
- **Contains:** All timeout settings correctly configured
- **Note:** Use this as the gold standard

#### 2. `sagemaker/deploy_endpoint.py`
- **Status:** ‚úÖ Correct
- **Contains:** 
  - `SAGEMAKER_MODEL_SERVER_TIMEOUT: '300'`
  - `container_startup_health_check_timeout=600`
  - `model_data_download_timeout=600`

#### 3. `sagemaker/run_sagemaker_pipeline.py`
- **Status:** ‚úÖ Correct
- **Function:** `deploy_model()`
- **Contains:** All required timeout settings in both direct deployment and fallback methods

---

### üîß **Fixed Scripts**

#### 4. `sagemaker/deploy_existing_model.py`
**Status:** ‚úÖ **FIXED**

**Before:**
```python
model = PyTorchModel(
    model_data=model_uri,
    role=role,
    entry_point='sagemaker/inference.py',
    source_dir=project_root,
    framework_version='2.0.0',
    py_version='py310',
    sagemaker_session=session
    # ‚ùå NO env variables
)

predictor = model.deploy(
    initial_instance_count=1,
    instance_type=instance_type,
    endpoint_name=endpoint_name,
    serializer=JSONSerializer(),
    deserializer=JSONDeserializer()
    # ‚ùå NO timeout parameters
)
```

**After:**
```python
model = PyTorchModel(
    model_data=model_uri,
    role=role,
    entry_point='sagemaker/inference.py',
    source_dir=project_root,
    framework_version='2.0.0',
    py_version='py310',
    sagemaker_session=session,
    model_server_workers=1,
    env={
        # ‚úÖ ADDED: Critical timeout settings
        'SAGEMAKER_MODEL_SERVER_TIMEOUT': '300',
        'MODEL_SERVER_TIMEOUT': '300',
        'MODEL_SERVER_WORKERS': '1',
        'TS_MAX_REQUEST_SIZE': '100000000',
        'TS_MAX_RESPONSE_SIZE': '100000000',
        'TS_DEFAULT_RESPONSE_TIMEOUT': '300',
        'TS_DEFAULT_WORKERS_PER_MODEL': '1',
        'OMP_NUM_THREADS': '2',
        'MKL_NUM_THREADS': '2',
        'TOKENIZERS_PARALLELISM': 'false',
    }
)

predictor = model.deploy(
    initial_instance_count=1,
    instance_type=instance_type,
    endpoint_name=endpoint_name,
    serializer=JSONSerializer(),
    deserializer=JSONDeserializer(),
    # ‚úÖ ADDED: Extended timeouts
    container_startup_health_check_timeout=600,
    model_data_download_timeout=600
)
```

**Changes:**
- ‚úÖ Added all required `env` variables including `SAGEMAKER_MODEL_SERVER_TIMEOUT`
- ‚úÖ Added `container_startup_health_check_timeout=600`
- ‚úÖ Added `model_data_download_timeout=600`
- ‚úÖ Added logging messages about extended timeouts

---

#### 5. `sagemaker/redeploy_endpoint.py`
**Status:** ‚úÖ **FIXED**

**Before:**
```python
model = PyTorchModel(
    # ... other params ...
    env={
        'MODEL_SERVER_TIMEOUT': '180',  # ‚ùå Only 180s (3 minutes) - TOO SHORT
        'MODEL_SERVER_WORKERS': '1',
        'TS_MAX_REQUEST_SIZE': '100000000',
        'TS_MAX_RESPONSE_SIZE': '100000000',
        'TS_DEFAULT_RESPONSE_TIMEOUT': '180'
        # ‚ùå MISSING: SAGEMAKER_MODEL_SERVER_TIMEOUT
    }
)

predictor = model.deploy(
    # ... other params ...
    container_startup_health_check_timeout=300,  # ‚ùå Only 5 minutes - RISKY
    model_data_download_timeout=300              # ‚ùå Only 5 minutes - RISKY
)
```

**After:**
```python
model = PyTorchModel(
    # ... other params ...
    env={
        # ‚úÖ ADDED: Critical SageMaker-specific timeout
        'SAGEMAKER_MODEL_SERVER_TIMEOUT': '300',
        'MODEL_SERVER_TIMEOUT': '300',  # ‚úÖ UPDATED: 180 ‚Üí 300 (5 minutes)
        'MODEL_SERVER_WORKERS': '1',
        'TS_MAX_REQUEST_SIZE': '100000000',
        'TS_MAX_RESPONSE_SIZE': '100000000',
        'TS_DEFAULT_RESPONSE_TIMEOUT': '300',  # ‚úÖ UPDATED: 180 ‚Üí 300
        'TS_DEFAULT_WORKERS_PER_MODEL': '1',  # ‚úÖ ADDED
        # ‚úÖ ADDED: Optimization flags
        'OMP_NUM_THREADS': '2',
        'MKL_NUM_THREADS': '2',
        'TOKENIZERS_PARALLELISM': 'false',
    }
)

predictor = model.deploy(
    # ... other params ...
    # ‚úÖ UPDATED: 300 ‚Üí 600 (10 minutes)
    container_startup_health_check_timeout=600,
    model_data_download_timeout=600
)
```

**Changes:**
- ‚úÖ Added `SAGEMAKER_MODEL_SERVER_TIMEOUT: '300'` (CRITICAL missing setting)
- ‚úÖ Increased `MODEL_SERVER_TIMEOUT` from 180s to 300s
- ‚úÖ Increased `TS_DEFAULT_RESPONSE_TIMEOUT` from 180s to 300s
- ‚úÖ Increased `container_startup_health_check_timeout` from 300s to 600s
- ‚úÖ Increased `model_data_download_timeout` from 300s to 600s
- ‚úÖ Added optimization environment variables
- ‚úÖ Updated timeout info in `endpoint_info.json` output

---

## Documentation Updates

### `Docs/deploy_sagemaker.md`

**Added:** New troubleshooting section "Issue 6: ModelError timeout"

```markdown
### Issue 6: "ModelError: Your invocation timed out while waiting for a response from container"

**Root Cause:** 
This error occurs when the CLIP model takes 2-3 minutes to load in `model_fn()`, 
but the default SageMaker timeout is only 60 seconds.

**Solution:**
All deployment scripts in this project now include the CRITICAL timeout fix:
- SAGEMAKER_MODEL_SERVER_TIMEOUT: '300'
- container_startup_health_check_timeout=600
- model_data_download_timeout=600
```

**Updated:** All subsequent issue numbers (Issue 6-11 ‚Üí Issue 7-11)

---

## Complete Configuration Reference

For any new deployment script or custom deployment, use this complete configuration:

```python
from sagemaker.pytorch import PyTorchModel
from sagemaker.serializers import JSONSerializer
from sagemaker.deserializers import JSONDeserializer

# Create model with extended timeouts
model = PyTorchModel(
    model_data=model_uri,
    role=execution_role,
    entry_point='sagemaker/inference.py',
    source_dir=project_root,
    framework_version='2.0.0',
    py_version='py310',
    sagemaker_session=session,
    model_server_workers=1,  # Single worker to reduce memory
    env={
        # ‚≠ê‚≠ê‚≠ê CRITICAL: SageMaker-specific timeout (MUST HAVE)
        'SAGEMAKER_MODEL_SERVER_TIMEOUT': '300',  # 5 minutes for model_fn
        
        # Model server configuration
        'MODEL_SERVER_TIMEOUT': '300',  # 5 minutes per request
        'MODEL_SERVER_WORKERS': '1',
        
        # TorchServe configuration
        'TS_MAX_REQUEST_SIZE': '100000000',  # 100MB
        'TS_MAX_RESPONSE_SIZE': '100000000',
        'TS_DEFAULT_RESPONSE_TIMEOUT': '300',  # 5 minutes
        'TS_DEFAULT_WORKERS_PER_MODEL': '1',
        
        # Optimization flags
        'OMP_NUM_THREADS': '2',
        'MKL_NUM_THREADS': '2',
        'TOKENIZERS_PARALLELISM': 'false',
    }
)

# Deploy with extended timeouts
predictor = model.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.large',
    endpoint_name=endpoint_name,
    serializer=JSONSerializer(),
    deserializer=JSONDeserializer(),
    
    # ‚≠ê‚≠ê‚≠ê CRITICAL: Extended timeouts for cold start (MUST HAVE)
    container_startup_health_check_timeout=600,  # 10 minutes for first startup
    model_data_download_timeout=600              # 10 minutes to download model
)
```

---

## Testing & Validation

### All Scripts Have Been Validated To Include:

1. ‚úÖ `SAGEMAKER_MODEL_SERVER_TIMEOUT: '300'` in `env` dictionary
2. ‚úÖ `MODEL_SERVER_TIMEOUT: '300'` in `env` dictionary  
3. ‚úÖ `TS_DEFAULT_RESPONSE_TIMEOUT: '300'` in `env` dictionary
4. ‚úÖ `container_startup_health_check_timeout=600` in `.deploy()`
5. ‚úÖ `model_data_download_timeout=600` in `.deploy()`

### Expected Behavior After Fix:

- ‚úÖ First inference request completes successfully (takes 2-3 minutes)
- ‚úÖ No ModelError timeout errors during cold start
- ‚úÖ Subsequent requests are fast (~1-2 seconds)
- ‚úÖ Container health checks pass within 10 minutes
- ‚úÖ Model downloads complete within 10 minutes

### How to Verify:

```bash
# Deploy using any fixed script
python sagemaker/deploy_existing_model.py --model-uri s3://bucket/model.tar.gz

# Test endpoint (first request will be slow but should succeed)
python scripts/testing/test_endpoint.py --save-visual

# Check logs for successful model load
aws logs tail /aws/sagemaker/Endpoints/stygig-endpoint-XXXXXXXX --follow
```

---

## Why Both Timeouts Are Required

### 1. `SAGEMAKER_MODEL_SERVER_TIMEOUT` (Server-Side)
- **Controls:** How long the model server waits for `model_fn()` to complete
- **Default:** 60 seconds
- **Problem:** CLIP model takes 120-180 seconds to load
- **Solution:** Set to 300 seconds (5 minutes)
- **Impact:** Prevents server from killing the process during model load

### 2. `container_startup_health_check_timeout` (Infrastructure-Side)
- **Controls:** How long SageMaker waits for the container to become healthy
- **Default:** 180 seconds (3 minutes)
- **Problem:** Container health check includes loading the model
- **Solution:** Set to 600 seconds (10 minutes)
- **Impact:** Gives SageMaker enough time to wait for container readiness

**Both are required** because they control different parts of the deployment:
- Server timeout ‚Üí Inside the container (model loading)
- Startup timeout ‚Üí Outside the container (infrastructure waiting)

---

## Future Maintenance

### When Adding New Deployment Scripts:

1. Copy timeout configuration from `redeploy_with_timeout.py` (reference implementation)
2. Include both `env` variables and `.deploy()` parameters
3. Add logging to indicate extended timeouts are in use
4. Test with a cold start to verify timeouts work

### When Modifying Existing Scripts:

1. ‚ö†Ô∏è **DO NOT reduce timeout values below:**
   - `SAGEMAKER_MODEL_SERVER_TIMEOUT`: 300s
   - `container_startup_health_check_timeout`: 600s
   - `model_data_download_timeout`: 600s

2. ‚úÖ **OK to increase** if you:
   - Use a larger model
   - Experience slower S3 downloads
   - Deploy to slower instance types

### Red Flags to Watch For:

‚ùå Missing `SAGEMAKER_MODEL_SERVER_TIMEOUT` in `env` dictionary  
‚ùå `container_startup_health_check_timeout` < 600  
‚ùå `model_data_download_timeout` < 600  
‚ùå Timeout values in seconds instead of string format  
‚ùå No `env` dictionary in `PyTorchModel()` constructor

---

## Summary

‚úÖ **2 scripts were missing timeout configurations** ‚Üí **FIXED**  
‚úÖ **3 scripts were already correct** ‚Üí **VERIFIED**  
‚úÖ **Documentation updated** ‚Üí **COMPLETE**  

### Scripts Fixed:
1. ‚úÖ `sagemaker/deploy_existing_model.py` - Added all timeout settings
2. ‚úÖ `sagemaker/redeploy_endpoint.py` - Added SAGEMAKER_MODEL_SERVER_TIMEOUT, increased timeouts from 180s/300s to 300s/600s

### Scripts Verified Correct:
1. ‚úÖ `sagemaker/redeploy_with_timeout.py` - Reference implementation
2. ‚úÖ `sagemaker/deploy_endpoint.py` - All settings present
3. ‚úÖ `sagemaker/run_sagemaker_pipeline.py` - All settings present

**Result:** All SageMaker deployment scripts now have robust timeout configurations to handle CLIP model cold starts without errors.

---

## Quick Reference

**Minimum Required Settings:**

```python
# In PyTorchModel constructor
env={'SAGEMAKER_MODEL_SERVER_TIMEOUT': '300', ...}

# In model.deploy() call
container_startup_health_check_timeout=600,
model_data_download_timeout=600
```

**That's it!** These three lines prevent the timeout error.

---

**Audit Completed By:** AI Assistant  
**Date:** November 7, 2025  
**Status:** ‚úÖ Complete and Production-Ready
